{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccba5c6c",
   "metadata": {},
   "source": [
    "# Trump GPT-2 Medium — Fine-Tuning on Google Colab\n",
    "\n",
    "Fine-tunes `gpt2-medium` on Trump speech data collected for ECE 595 NLP Assignment 01.\n",
    "\n",
    "**Before running:**\n",
    "1. Set runtime to **T4 GPU**: `Runtime → Change runtime type → T4 GPU`\n",
    "2. Upload `trump_train.txt` when prompted in the *Load Dataset* cell (or place it in Drive first)\n",
    "3. Run all cells top-to-bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a233b2",
   "metadata": {},
   "source": [
    "## 1 · Setup — Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install -q transformers datasets accelerate soundfile scipy\n",
    "\n",
    "import os, sys, torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2TokenizerFast,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "print(f\"PyTorch  : {torch.__version__}\")\n",
    "print(f\"CUDA     : {torch.cuda.is_available()}  —  {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU only'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812ec91",
   "metadata": {},
   "source": [
    "## 2 · Mount Google Drive\n",
    "\n",
    "Mounting Drive lets checkpoints and the final model survive session disconnects.\n",
    "After training, the model will be saved to `MyDrive/ECE595/models/trump/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63796bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/ECE595\"\n",
    "os.makedirs(f\"{DRIVE_ROOT}/data\", exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_ROOT}/models/trump\",   exist_ok=True)\n",
    "print(\"Drive mounted. Project root:\", DRIVE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3040e",
   "metadata": {},
   "source": [
    "## 3 · Configure Training Parameters\n",
    "\n",
    "All hyperparameters are centralised here — edit this cell to tune the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Paths ─────────────────────────────────────────────────────────────────────\n",
    "TRAIN_DATA = f\"{DRIVE_ROOT}/data/trump_train.txt\"\n",
    "OUTPUT_DIR = f\"{DRIVE_ROOT}/models/trump\"\n",
    "\n",
    "# ── Hyperparameters ────────────────────────────────────────────────────────────\n",
    "BASE_MODEL  = \"gpt2-medium\"   # 345 M params\n",
    "EPOCHS      = 8\n",
    "BATCH_SIZE  = 4               # safe for T4 (16 GB); raise to 8 on A100\n",
    "GRAD_ACCUM  = 4               # effective batch = BATCH_SIZE × GRAD_ACCUM = 16\n",
    "LEARN_RATE  = 5e-5\n",
    "BLOCK_SIZE  = 512\n",
    "RESUME      = False           # set True to continue from last checkpoint\n",
    "USE_FP16    = True            # auto-disabled on CPU\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device   : {DEVICE}\")\n",
    "print(f\"Data     : {TRAIN_DATA}\")\n",
    "print(f\"Output   : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02636b",
   "metadata": {},
   "source": [
    "## 4 · Load and Preprocess Dataset\n",
    "\n",
    "Upload `trump_train.txt` from your local machine **or** copy it to Drive manually first.  \n",
    "If the file is already in Drive (at the path configured above) the upload step is skipped automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Upload from local disk if not already on Drive ────────────────────────────\n",
    "if not os.path.isfile(TRAIN_DATA):\n",
    "    from google.colab import files\n",
    "    print(\"trump_train.txt not found on Drive — upload it now:\")\n",
    "    uploaded = files.upload()                     # opens file picker\n",
    "    fname = list(uploaded.keys())[0]\n",
    "    with open(TRAIN_DATA, \"wb\") as f:\n",
    "        f.write(uploaded[fname])\n",
    "    print(f\"Saved to {TRAIN_DATA}\")\n",
    "else:\n",
    "    print(f\"Found existing data file: {TRAIN_DATA}\")\n",
    "\n",
    "# ── Tokenise into fixed-length blocks ─────────────────────────────────────────\n",
    "def build_dataset(tokenizer, file_path: str, block_size: int):\n",
    "    print(f\"\\nLoading: {file_path}\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    examples = [e.strip() for e in raw_text.split(\"\\n\\n\") if e.strip()]\n",
    "    print(f\"Raw examples : {len(examples):,}\")\n",
    "\n",
    "    def tokenize(batch):\n",
    "        tokens = tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=block_size,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "        return tokens\n",
    "\n",
    "    ds = Dataset.from_dict({\"text\": examples})\n",
    "    ds = ds.map(tokenize, batched=True, remove_columns=[\"text\"], desc=\"Tokenising\")\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a52ce8",
   "metadata": {},
   "source": [
    "## 5 · Build Model and Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4361f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Loading tokeniser from  {BASE_MODEL}  …\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<|startoftext|>\", \"<|endoftext|>\",\n",
    "        \"[BIDEN]:\", \"[TRUMP]:\",\n",
    "    ]\n",
    "})\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Loading model  {BASE_MODEL}  …\")\n",
    "model = GPT2LMHeadModel.from_pretrained(BASE_MODEL)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f\"Model parameters : {total_params:.0f} M\")\n",
    "print(f\"Vocab size       : {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392cb3a2",
   "metadata": {},
   "source": [
    "## 6 · Training Loop\n",
    "\n",
    "Runs via HuggingFace `Trainer`. Estimated time on a **free T4**: ~10–20 min total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = build_dataset(tokenizer, TRAIN_DATA, BLOCK_SIZE)\n",
    "split    = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds = split[\"train\"]\n",
    "eval_ds  = split[\"test\"]\n",
    "print(f\"Train : {len(train_ds):,}  |  Eval : {len(eval_ds):,}\")\n",
    "\n",
    "# Auto-compute logging_steps so we always get at least one log per epoch\n",
    "steps_per_epoch = max(1, len(train_ds) // (BATCH_SIZE * GRAD_ACCUM))\n",
    "log_every       = max(1, steps_per_epoch // 4)   # ~4 logs per epoch\n",
    "print(f\"Steps/epoch: {steps_per_epoch}  →  logging every {log_every} steps\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                  = OUTPUT_DIR,\n",
    "    num_train_epochs            = EPOCHS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size  = BATCH_SIZE,\n",
    "    gradient_accumulation_steps = GRAD_ACCUM,\n",
    "    learning_rate               = LEARN_RATE,\n",
    "    weight_decay                = 0.01,\n",
    "    warmup_steps                = 16,\n",
    "    lr_scheduler_type           = \"cosine\",\n",
    "    eval_strategy               = \"epoch\",\n",
    "    save_strategy               = \"epoch\",\n",
    "    load_best_model_at_end      = True,\n",
    "    metric_for_best_model       = \"eval_loss\",\n",
    "    greater_is_better           = False,\n",
    "    fp16                        = USE_FP16 and torch.cuda.is_available(),\n",
    "    dataloader_pin_memory       = torch.cuda.is_available(),\n",
    "    logging_steps               = log_every,\n",
    "    report_to                   = \"none\",\n",
    "    seed                        = 42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model         = model,\n",
    "    args          = training_args,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset  = eval_ds,\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "resume_ckpt = OUTPUT_DIR if RESUME and os.path.isdir(OUTPUT_DIR) else None\n",
    "print(\"\\nStarting training …\")\n",
    "trainer.train(resume_from_checkpoint=resume_ckpt)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49bd5b",
   "metadata": {},
   "source": [
    "## 7 · Evaluate Model Performance\n",
    "\n",
    "Plot the train/eval loss curves logged during training and run a quick perplexity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83ab062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ── Loss curves ───────────────────────────────────────────────────────────────\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "train_steps  = [e[\"step\"] for e in log_history if \"loss\" in e]\n",
    "train_losses = [e[\"loss\"] for e in log_history if \"loss\" in e]\n",
    "eval_epochs  = [e[\"epoch\"] for e in log_history if \"eval_loss\" in e]\n",
    "eval_losses  = [e[\"eval_loss\"] for e in log_history if \"eval_loss\" in e]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "ax.plot(train_steps, train_losses, label=\"Train loss\", alpha=0.8)\n",
    "if eval_losses:\n",
    "    ax2 = ax.twiny()\n",
    "    ax2.plot(eval_epochs, eval_losses, \"o--\", color=\"tomato\", label=\"Eval loss\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "ax.set_xlabel(\"Step\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Trump GPT-2 — Training Loss\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ── Final perplexity ──────────────────────────────────────────────────────────\n",
    "final_eval = trainer.evaluate()\n",
    "perplexity = math.exp(final_eval[\"eval_loss\"])\n",
    "print(f\"\\nFinal eval loss : {final_eval['eval_loss']:.4f}\")\n",
    "print(f\"Perplexity      : {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60000a67",
   "metadata": {},
   "source": [
    "## 8 · Save and Export Model\n",
    "\n",
    "The `Trainer` already saves checkpoints to Drive after every epoch.  \n",
    "This cell does a final explicit save and verifies the files are intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Final save ────────────────────────────────────────────────────────────────\n",
    "print(f\"Saving model to {OUTPUT_DIR} …\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# ── Verify saved files ────────────────────────────────────────────────────────\n",
    "saved_files = os.listdir(OUTPUT_DIR)\n",
    "print(\"\\nSaved files:\")\n",
    "for f in sorted(saved_files):\n",
    "    size = os.path.getsize(os.path.join(OUTPUT_DIR, f)) / 1e6\n",
    "    print(f\"  {f:<40} {size:.1f} MB\")\n",
    "\n",
    "# ── Quick sample generation (sanity check) ────────────────────────────────────\n",
    "print(\"\\n── Sample generation ──────────────────────────────────────\")\n",
    "prompt = \"[TRUMP]: Nobody knows more about this than me —\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=80,\n",
    "        do_sample=True,\n",
    "        temperature=0.85,\n",
    "        top_p=0.92,\n",
    "        repetition_penalty=1.3,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "print(\"\\n[DONE] Trump model saved to:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e596bbe",
   "metadata": {},
   "source": [
    "## 9 · Download Model to Local Machine\n",
    "\n",
    "The model folder contains several files (`model.safetensors`, `config.json`, tokenizer files, etc.).  \n",
    "This cell zips the entire folder into **one file** and downloads it straight to your browser's download folder.\n",
    "\n",
    "After downloading, extract it into `models/trump/` inside your local project so `start_debate.py` can find it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b73908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "ZIP_PATH = \"/content/trump_model.zip\"\n",
    "\n",
    "print(f\"Zipping {OUTPUT_DIR} → {ZIP_PATH} …\")\n",
    "with zipfile.ZipFile(ZIP_PATH, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for fname in os.listdir(OUTPUT_DIR):\n",
    "        fpath = os.path.join(OUTPUT_DIR, fname)\n",
    "        if os.path.isfile(fpath):\n",
    "            zf.write(fpath, arcname=fname)\n",
    "            size_mb = os.path.getsize(fpath) / 1e6\n",
    "            print(f\"  + {fname:<45} {size_mb:.1f} MB\")\n",
    "\n",
    "zip_size_mb = os.path.getsize(ZIP_PATH) / 1e6\n",
    "print(f\"\\nZip size : {zip_size_mb:.1f} MB\")\n",
    "print(\"Downloading …\")\n",
    "files.download(ZIP_PATH)\n",
    "print(\"\\n[DONE]  Extract the zip into  models/trump/  in your local project.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
